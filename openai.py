import os
import json
import requests
import time

# API Configuration
API_URL = "https://openweb-ui.moengage.ai/api/chat/completions"
API_KEY = "sk-ffdbf96f654f405b8511165aac6bce72"  # Replace this with your actual API key.
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}

# Directory to save chat history
CHAT_HISTORY_DIR = "chat_history"
os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)


def retry_on_failure(func, retries=3, delay=10):
    """
    Retries a function in case of failure.

    Args:
        func (callable): Function to retry. Should not take arguments.
        retries (int): Number of times to retry before giving up.
        delay (int): Delay between retries in seconds.

    Returns:
        Result of the function, or None if all retries fail.
    """
    for attempt in range(1, retries + 1):
        try:
            return func()  # Execute the function
        except requests.exceptions.RequestException as e:
            # This handles network-related errors
            print(f"\nError: {e}. Retrying in {delay} seconds ({attempt}/{retries})...")
        except KeyError as e:
            print(f"\nUnexpected response format: {e}. Retrying in {delay} seconds ({attempt}/{retries})...")

        # Countdown for retry
        for sec in range(delay, 0, -1):
            print(f"\rRetrying in {sec} seconds...", end="", flush=True)
            time.sleep(1)

    print("\nAll retries exhausted. Skipping.")
    return None


def send_message_with_context(model, messages):
    """
    Sends a message to the API with accumulated context.

    Args:
        model (str): The AI model to use (e.g., "gpt-4o").
        messages (list): The list of messages comprising context.

    Returns:
        str: Assistant's reply, or None if the request ultimately fails.
    """

    def api_request():
        data = {"model": model, "messages": messages, "stream": False}
        response = requests.post(API_URL, headers=HEADERS, json=data)
        response.raise_for_status()  # Raise an error for non-success HTTP codes
        reply = response.json()
        return reply["choices"][0]["message"]["content"]

    # Perform the API call with retry logic
    return retry_on_failure(api_request)


def process_query_with_s0(user_query):
    """
    Module S0: Improves the quality of the user's query.

    Args:
        user_query (str): The raw user query.

    Returns:
        str: The improved query, or None if it fails entirely.
    """
    model_name = "gpt-4o"
    system_message = {
        "role": "system",
        "content": """You are an advanced language model specifically designed to transform structured logical 
        conditions into concise, clear, and fluent natural language. Your primary goal is to maintain the original 
        intent, logical accuracy, and meaning of the input while ensuring ease of understanding for a human audience. 
        Follow these guidelines: Use natural, conversational language. Avoid adding unnecessary details, assumptions, 
        or deviating from the intended meaning. Maintain logical precision and fluency in all output. Before 
        responding, evaluate whether you fully understand the provided context and query. If you comprehend the input 
        clearly and are ready to deliver the final response, begin your answer with the prefix: 'Final:'. If you 
        require additional context or clarification, respond with the prefix: 'Understanding:' followed by what 
        further information is needed. Your output should adhere strictly to these instructions. Do not skip steps or 
        deviate from the guidelines."""
    }
    messages = [
        system_message,
        {"role": "user", "content": user_query}
    ]

    # Retry query improvement on failure
    improved_query = retry_on_failure(lambda: send_message_with_context(model_name, messages))
    return improved_query


def process_query_with_s1(improved_query, conversation, context):
    """
    Module S1: Processes the improved query with context and returns a structured response.

    Args:
        improved_query (str): The improved user query generated by S0.
        conversation (list): The conversation history or additional context messages.
        context (str): The context to be used for the structured response.

    Returns:
        str: The structured output, or None if it fails entirely.
    """
    model_name = "gpt-4o"
    system_message = {
        "role": "system",
        "content": """You are an advanced system tasked with processing user queries by converting them into either: 
        Structured Steps (if the query involves a detailed procedure or process to be communicated) using the prefix 
        'Steps:', or Concise Explanations (if the query requires brief and clear information about a topic) using the 
        prefix 'Explanation:'. Key Guidelines: Focus strictly on the intent of the query, regardless of tone, 
        wording, phrasing, typos, spelling errors, slang, or informal language. Extract and convey only meaningful, 
        actionable, or informative content aligned with the user’s request. Maintain a consistent structure and 
        formatting to ensure outputs are clear, regardless of how the input is phrased. Avoid: interpreting emotions, 
        opinions, or unrelated requests; adding additional context not explicitly required; or deviating from the 
        intended logic. Ensure that a query expressing the same intent always produces the exact same structured 
        output, no matter the style or variation in input phrasing. Outputs should remain error-free and logically 
        coherent. You must strictly follow the above rules for every response. If a procedure is identified, 
        begin with 'Steps:'; if a topic explanation is identified, begin with 'Explanation:'. Avoid including 
        extraneous details or ambiguities."""
    }

    context_message = {
        "role": "system",
        "content": "This is the full extent of the context available to address the user's query. Use this context "
                   "exclusively to generate your response. If the provided context is sufficient to fully understand "
                   "and answer the query, proceed as instructed while adhering strictly to logical accuracy and user "
                   "intent. If additional context is required to produce a complete and precise response, "
                   "indicate this explicitly and request further clarification. Refrain from making assumptions or "
                   "introducing information not present in the provided context. Maintain clarity, precision, "
                   "and relevance at all times. " + context
    }

    messages = [context_message] + [system_message] + conversation + [{"role": "user", "content": improved_query}]

    # Retry structured response on failure
    structured_output = retry_on_failure(lambda: send_message_with_context(model_name, messages))
    return structured_output


def save_chat_history(history):
    """
    Saves the entire chat history to a file.

    Args:
        history (list): The complete conversation history (list of messages).
    """
    filename = input("\nEnter a filename to save this conversation (e.g., 'chat1.json'): ")
    filepath = os.path.join(CHAT_HISTORY_DIR, filename)
    with open(filepath, "w") as f:
        json.dump(history, f, indent=4)
    print(f"Conversation saved to {filepath}.")


def new_conversation():
    """
    Starts a new modular conversation (S0 → S1) and allows continuation with context.
    """
    print("Starting a new modular conversation. Type 'exit' to end.")
    conversation = []  # List to track query and assistant responses

    history = []  # Clean history: [Improved Query + Structured Output]
    while True:
        user_query = input("\nYou: ")
        if user_query.lower() in ["exit", "quit"]:
            save_chat_history(history)
            print("Exiting the conversation.")
            break

        # Step 1: Improve the query using S0
        print("\n[Processing query with S0 to improve its quality...]")
        improved_query = process_query_with_s0(user_query)
        if not improved_query:
            print("\n[S0] Failed to improve query after retries. Moving to the next interaction.")
            continue
        print(f"\nS0 Improved Query: {improved_query}")

        # Step 2: Process the improved query with context using S1
        print("\n[Processing improved query with S1 for a structured response...]")
        structured_output = process_query_with_s1(improved_query, conversation)
        if not structured_output:
            print("\n[S1] Failed to generate structured response after retries. Moving to the next interaction.")
            continue
        print(f"\nS1 Structured Output:\n{structured_output}")

        # Step 3: Append to context and save clean history
        conversation.append({"role": "user", "content": improved_query})
        conversation.append({"role": "assistant", "content": structured_output})
        history.append({"Improved Query": improved_query, "Structured Output": structured_output})


def list_saved_conversations():
    """
    Lists all saved conversations in the chat history directory.

    Returns:
        list: A list of file names.
    """
    files = [f for f in os.listdir(CHAT_HISTORY_DIR) if f.endswith(".json")]
    if not files:
        print("No saved conversations found.")
    else:
        print("\nSaved Conversations:")
        for i, file in enumerate(files, 1):
            print(f"{i}. {file}")
    return files


def load_and_continue_conversation():
    """
    Loads a saved conversation and continues it.
    """
    files = list_saved_conversations()
    if not files:
        return
    choice = int(input("\nEnter the number of the conversation to continue: ")) - 1
    filepath = os.path.join(CHAT_HISTORY_DIR, files[choice])
    with open(filepath, "r") as f:
        loaded_data = json.load(f)

    conversation = []
    for item in loaded_data:
        conversation.append({"role": "user", "content": item["Improved Query"]})
        conversation.append({"role": "assistant", "content": item["Structured Output"]})

    print("Continuing the conversation. Type 'exit' to end.")
    while True:
        user_query = input("\nYou: ")
        if user_query.lower() in ["exit", "quit"]:
            save_chat_history(loaded_data)
            print("Exiting the conversation.")
            break

        # Step 1: Improve query with S0
        improved_query = process_query_with_s0(user_query)
        if not improved_query:
            print("\n[S0] Failed to improve the query after retries. Moving to the next interaction.")
            continue

        # Step 2: Process with S1
        structured_output = process_query_with_s1(improved_query, conversation)
        if not structured_output:
            print("\n[S1] Failed to get structured response after retries. Moving to the next interaction.")
            continue

        # Update context and history
        conversation.append({"role": "user", "content": improved_query})
        conversation.append({"role": "assistant", "content": structured_output})
        loaded_data.append({"Improved Query": improved_query, "Structured Output": structured_output})


def main_menu():
    """
    Displays the main menu for the chat application.
    """
    while True:
        print("\n--- Modular Chat Application Menu ---")
        print("1. Start a new conversation")
        print("2. Continue a saved conversation")
        print("3. View saved conversations")
        print("4. Exit")
        choice = input("Select an option: ")

        if choice == "1":
            new_conversation()
        elif choice == "2":
            load_and_continue_conversation()
        elif choice == "3":
            list_saved_conversations()
        elif choice == "4":
            print("Exiting the application. Goodbye!")
            break
        else:
            print("Invalid choice. Please try again.")


if __name__ == "__main__":
    main_menu()
